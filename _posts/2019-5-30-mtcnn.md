---
layout: post
title: Multi-task Cascade Convolutional Network (MTCNN) 
subtitle: Kiến trúc và cách hoạt động của MTCNN, một trong những mạng convolutional hoạt động tốt nhất trong face detection hiện nay.
tags: []
comments: true
---
*Multi-task Cascade Convolutional Network (MTCNN) được sử dụng rất phổ biến khi đề cập tớt face detection, bởi độ chính xác cao của nó. Hôm nay mình sẽ giới thiệu với mọi người về kiến trúc cơ bản nhất của mạng này và cách nó hoạt động.* 

MTCNN hoạt động theo 3 stages, tại mỗi stage sẽ có một mạng neural riêng lần lượt là : P-Net, R-Net và O-Net. Chúng ta cùng xem xét từng stage và mạng neural lần lượt.

![Cấu trúc mạng MTCNN. Có 3 stages chính, mỗi stage có một mạng neural tương ứng](https://images.viblo.asia/34406778-660d-4bbf-918b-a087e193eba6.png)

##Stage 1
Việc đầu tiên cần làm hiển nhiên là đưa một bức ảnh làm đầu vào của mạng neural, ở stage 1 đó là P-Net. Trong model này, chúng ta một tạo ra các ***image pyramid*** (kim tự tháp hình ảnh) để xác định khuôn mặt với tất cả các kích cỡ khác nhau. Nói cách khác là chúng ta muốn tạo ra nhiều bản sao của cùng một bức ảnh ở nhiều kích thước khác nhau, rồi tiến hành tìm kiếm nhiều khuôn mặt với kích thước tương ứng trong mỗi bức ảnh. 

![Image Pyramid](https://cdn-images-1.medium.com/max/800/1*JH-L5EmTqj_fHEcXnzZT5Q.png)

Tại P-Net, thuật toán sử dụng 1 kernel 12x12 chạy qua tất cả các phần của bức hình để tìm kiếm khuôn mặt. Nó bắt đầu từ phần  góc trên bên trái của bức ảnh với tọa độ từ (0,0) đến (12,12). Phần này của bức ảnh được đưa qua P-Net, rồi sẽ trả về tọa độ của một ***bounding box*** nếu nó cho rằng có mặt nguời xuất hiện. Rồi cứ thế nó sẽ lặp lại quá trình trên với phần có tọa độ (0+2a, 0+2b) đến (12+2a, 12+2b), dịch kernel 12x12 2 pixels sang phải hoặc xuống dưới cùng một lúc. Khoảng cách này được gói là ***stride*** nếu bạn chưa biết, chính là số pixels mà kernel dịch chuyển mỗi lần.

Stride 2 sẽ giúp chúng ta giảm thiểu được chi phí tính toán mà không làm mất đi độ chính xác của quá trình detect. Vì hầu hết các khuôn mặt thì đều có kích thước lớn hơn 2 pixels khá nhiều, nên gần như không thể xảy ra việc kernel sẽ bỏ sót một khuôn mặt nào đó chỉ bởi việc dịch chuyển 2 pixels, trừ khi nó ở rất xa trong bức ảnh, và như thế thì mắt người cũng khó mà xác định được. Đồng thời thì máy tính, hay bất cứ thiết bị nào mà bạn đang dùng để chạy model, có thể giảm được khối lượng cần tính toán đi 4 lần, giúp cho model có thể inference nhanh hơn và tiết kiệm bộ nhớ hơn.

Nhược điểm duy nhất chính là việc chúng ta phải tính toán lại tất cả các dữ kiện liên quan đến stride. Ví dụ, nếu một kernel xác định được một khuôn mặt sau khi di chuyển một bước sang bên phải, chỉ số đầu ra sẽ cho chúng ta biết tọa độ góc trên bên trái của kernel đó tại (1,0). Tuy nhiên, do stride là 2, chúng ta sẽ phải nhân chỉ với 2 để được tọa độ chính xác: (2,0).

Mỗi kernel sẽ nhỏ hơn nếu nằm trong một bức ảnh lớn hơn, do đó nó sẽ tìm được các khuôn mặt nhỏ hơn trong bức ảnh có kích thước lớn hơn. Tương tự, kernel sẽ lớn hơn nếu nằm trong một bức ảnh có kích thước nhỏ hơn, do đó có thể tìm được khuôn mặt to trong những bức ảnh kích thước nhỏ.

![Stage 1: P-Net](https://images.viblo.asia/c2ad8543-8c46-4485-a92c-ca3a8b893b7a.png)

Sau lớp convolution thứ 3, mạng chia thành 2 lớp. Convolution 4-1 đưa ra xác suất của một khuôn mặt nằm trong mỗi bounding boxes, và Convolution 4-2 cung cấp tọa độ của các bounding boxes.

Và dưới đây là output:

![Output cho P-Net. Chú ý rằng, output thực sự có 4 chiều, tuy nhiên để đơn giản thì ở đây tác giả đã kết hợp thành một mảng 2 chiều. Đồng thời, tọa độ của các bouding boxes là giá trị nằm giữa 0 và 1: (0,0) là góc trên bên trái của kernel, (1,1) là góc dưới bên phải của kernel.](https://cdn-images-1.medium.com/max/800/1*5jhmkluZtYXUbzfi-cplIQ.png)

Các ***weights*** và ***biases*** của P-Net đã được huấn luyện nên nó có thể cho ra các bouding box tương đối chính xác cho mỗi kernel 12x12. Tuy nhiên, trong số các bouding boxes đó có một số boxes mà mạng cho rằng có xác suất cao hơn, hay nó dự đoán chắc chắn hơn. Vì thế, chúng ta cần đưa output của P-Net về một list chứa các mứa độ chắc chắc của mỗi bouding box, và sau đó xóa đi các boxes với ***confidence*** thấp. 

![](https://cdn-images-1.medium.com/max/800/1*Ml1270kY5DoN4vU9Uwwi-A.png)

Sau khi chọn ra các boxes với ***confidence*** cao nhất, chúng ta sẽ phải chuẩn hóa hệ tọa độ, chuyển tất cả các hệ tọa độ về dạng thật của nó, ở bức ảnh gốc khi chưa được scaled sang các kích cỡ khác nhau. Vì phần lớn các kernels đang ở trong các bức ảnh được scale xuống các kích cỡ thấp hơn nên tọa độ của chúng đang được dựa vào các bức ảnh nhỏ hơn.

Tuy nhiên vẫn còn rất nhiều các bouding boxes trong bức ảnh, và rất nhiều trong số chúng nằm trùng lên nhau. Ở đây, ta sẽ sử dụng ***Non-Maximum Suppression***, hay NMS, là phương pháp để giảm số lượng các bouding boxes nằm trùng lên nhau.
Bạn đọc có thể tìm hiểu thêm về phương pháp này tại [đây](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) hoặc [đây](https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/).

![Non-Maximum Suppression](https://cdn-images-1.medium.com/max/800/1*Ml1270kY5DoN4vU9Uwwi-A.png)

Chúng ta thực hiện NMS cho tất cả các bức ảnh được scale, rồi thêm một lần nữa cho tất cả các kernels ở mỗi scale. Việc này giúp bỏ đi các bouding boxes thừa, cho phép chúng ta thu hẹp phạm vi tìm kiếm xuống còn một box chính xác nhất cho mỗi khuôn mặt.

Có thể đến đây sẽ có nhiều người hỏi rằng tại sao không chọn luôn box với ***confidence*** cao nhất và xóa đi tất cả những cái còn lại? Trong bức ảnh trên của chúng ta chỉ có một khuôn mặt, tuy nhiên có thể trong các bức ảnh khác lại có nhiều khuôn mặt hơn. Vì vậy, chúng ta có thể sẽ xóa đi bouding boxes của tất cả các khuôn mặt khác có trong bức ảnh. 

![Thực hiện đưa bức ảnh về kích thước gốc](https://cdn-images-1.medium.com/max/800/1*7JWQP4FhkMxuCfzSsfFR_A.png)

Trong bức ảnh trên, box màu đỏ biểu diễn kernel 12x12, được resized về bức ảnh gốc ban đầu. Chúng ta có thể tính được chiều rộng và chiều dài của kernel: 500 - 200 = 300, 1800 - 1500 = 300 ( Để ý rằng chiều dài và chiều rộng của kernel không nhất thiết phải là 12. Đó là bởi vi ta đang sử dụng tọa độ của kernel trong bức ảnh gốc. Chiều dài và chiều rộng của kernel ở đây là khi đã được đưa về kích thước của bức ảnh gốc) Sau đó, chúng ta nhân tọa độ của bounding box màu vàng với 300: 0.4x300 = 120, 0.2x300=60, 0.9x300=270, 0.7x300=210. Để ý ở đây tọa độ của bounding box đã được chuẩn hóa về giữa 0 và 1 so với hệ tọa độ trong kernel. Cuối cùng, ta cộng tọa độ phía trên bên trái của kernel để được tọa độ chính xác của bouding box: (200+120,500+60) và (200+270,500+210) hay (320,560) và (470,710).

Vì bounding box có thể không là hình vuông, nên chúng ta sẽ reshape chúng về hình vuông bằng cách kéo dài cạnh nhỏ hơn ra. Sau đó, ta lưu lại tọa độ của bouding box và chuyển sang stage 2.

## Stage 2
Trong một số trường hợp, khuôn mặt có thể không nằm trọn hoàn toàn trong bức ảnh mà bị mất một phần. Trong trường hợp đó, mạng neural của chúng ta có thể trả về bounding box nằm ngoài bức ảnh, như khuôn mặt trong bức ảnh dưới đây:

![](https://cdn-images-1.medium.com/max/800/1*yoZ7DuNCvH64jY6xKFpO7g.png) 

Cho mỗi bouding box, ta tạo một mảng có cùng kích thước, và copy các giá trị pixels (ảnh ở trong bouding box) vào mảng mới. Nếu bouding box bị lấn lề, chúng ta sẽ chỉ copy phần của bức ảnh trong bounding box vào mảng mới và lấp đầy phần còn lại bằng giá trị 0. Quá trình lấp đầy mảng bởi các giá trị 0 này được gọi là ***padding***.

Say đó, bouding box sau khi được padding sẽ được resize về 24x24 pixels, và chuẩn hóa giá trị về trong khoảng -1 đến 1. Trước đó, giá trị các pixels nằm trong khoảng 0 đến 255 (giá trị của hệ RGB). Ta chuẩn hóa bằng cách với mỗi giá trị pixel, ta trừ đi 127.5 rồi chia cho 127.5.

Giờ chúng ta có các mảng 24x24, chính là số các bounding boxes còn ots lại qua Stage 1, ta đưa chúng qua mạng R-Net và lấy outputs.

R-Net có cấu trúc tương tự vói P-Net. Tuy nhiên sử dụng nhiều layer hơn. Tại đây, network sẽ sử dụng các bounding boxes đc cung cấp từ P-Net và tinh chỉnh là tọa độ.
![R-Net](https://images.viblo.asia/b2a0e943-283b-4fe5-8feb-133771011982.png)

Tương tự R-Net chia ra làm 2 layers ở bước cuối,cung cấp 2 đầu ra đó là tọa độ mới của các bounding boxes, cùng độ tin tưởng của nó. Các bước NMS và đưa về hình vuông với các bounding boxes được thực hiện tương tự nhử với P-Net. 

Cuối cùng, ta đưa tọa độ của output về dạng chuẩn và đưa vào O-Net.

##Stage 3
Trước khi đưa các bounding boxes từ R-Net vào O-Net, ta sẽ thực hiện ***padding*** và reshape chúng về 48x48 pixels.

O-Net lấy các bounding boxes từ R-Net làm đầu vào và đánh dấu các tọa độ của các mốc trên khuôn mặt.

![O-Net](https://images.viblo.asia/9f85936e-1541-4865-9768-7a53cf5841ee.png)

Ở bước này, thuật toán đưa ra 3 kết quả đầu ra khác nhau bao gồm: xác suất của khuôn mặt nằm trong bounding box, tọa độ của bounding box và tọa độ của các mốc trên khuôn mặt (vị trí mắt, mũi, miệng)

![](https://images.viblo.asia/335d51c9-e9b5-4c19-936d-1874806de71d.png)

```
Trên đây là toàn bộ bài viết về Multi-task Cascaded Convolutional Network của mình. Cảm ơn mọi người đã đọc hết. Rất mong nhận được sự đóng góp của mọi người để bài viết được hoàn thiện hơn nữa. Hẹn gặp lại trong các bài viết sau nhé.
```
## Tài liệu tham khảo
1. https://towardsdatascience.com/how-does-a-face-detection-program-work-using-neural-networks-17896df8e6ff
2. https://viblo.asia/p/tim-hieu-mtcnn-va-ap-dung-de-xac-dinh-vi-tri-cac-khuon-mat-3Q75wkO75Wb
3. https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf
