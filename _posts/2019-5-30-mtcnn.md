---
layout: post
title: Multi-task Cascade Convolutional Network (MTCNN) 
subtitle: Kiến trúc và cách hoạt động của MTCNN, một trong những mạng convolutional hoạt động tốt nhất trong face detection hiện nay.
tags: []
comments: true
---
*Multi-task Cascade Convolutional Network (MTCNN) được sử dụng rất phổ biến khi đề cập tớt face detection, bởi độ chính xác cao của nó. Hôm nay mình sẽ giới thiệu với mọi người về kiến trúc cơ bản nhất của mạng này và cách nó hoạt động.* 

MTCNN hoạt động theo 3 stages, tại mỗi stage sẽ có một mạng neural riêng lần lượt là : P-Net, R-Net và O-Net. Chúng ta cùng xem xét từng stage và mạng neural lần lượt.

![Cấu trúc mạng MTCNN. Có 3 stages chính, mỗi stage có một mạng neural tương ứng](https://images.viblo.asia/34406778-660d-4bbf-918b-a087e193eba6.png)

##Stage 1
Việc đầu tiên cần làm hiển nhiên là đưa một bức ảnh làm đầu vào của mạng neural, ở stage 1 đó là P-Net. Trong model này, chúng ta một tạo ra các ***image pyramid*** (kim tự tháp hình ảnh) để xác định khuôn mặt với tất cả các kích cỡ khác nhau. Nói cách khác là chúng ta muốn tạo ra nhiều bản sao của cùng một bức ảnh ở nhiều kích thước khác nhau, rồi tiến hành tìm kiếm nhiều khuôn mặt với kích thước tương ứng trong mỗi bức ảnh. 

![Image Pyramid](https://cdn-images-1.medium.com/max/800/1*JH-L5EmTqj_fHEcXnzZT5Q.png)

Tại P-Net, thuật toán sử dụng 1 kernel 12x12 chạy qua tất cả các phần của bức hình để tìm kiếm khuôn mặt. Nó bắt đầu từ phần  góc trên bên trái của bức ảnh với tọa độ từ (0,0) đến (12,12). Phần này của bức ảnh được đưa qua P-Net, rồi sẽ trả về tọa độ của một ***bounding box*** nếu nó cho rằng có mặt nguời xuất hiện. Rồi cứ thế nó sẽ lặp lại quá trình trên với phần có tọa độ (0+2a, 0+2b) đến (12+2a, 12+2b), dịch kernel 12x12 2 pixels sang phải hoặc xuống dưới cùng một lúc. Khoảng cách này được gói là ***stride*** nếu bạn chưa biết, chính là số pixels mà kernel dịch chuyển mỗi lần.

Stride 2 sẽ giúp chúng ta giảm thiểu được chi phí tính toán mà không làm mất đi độ chính xác của quá trình detect. Vì hầu hết các khuôn mặt thì đều có kích thước lớn hơn 2 pixels khá nhiều, nên gần như không thể xảy ra việc kernel sẽ bỏ sót một khuôn mặt nào đó chỉ bởi việc dịch chuyển 2 pixels, trừ khi nó ở rất xa trong bức ảnh, và như thế thì mắt người cũng khó mà xác định được. Đồng thời thì máy tính, hay bất cứ thiết bị nào mà bạn đang dùng để chạy model, có thể giảm được khối lượng cần tính toán đi 4 lần, giúp cho model có thể inference nhanh hơn và tiết kiệm bộ nhớ hơn.

Nhược điểm duy nhất chính là việc chúng ta phải tính toán lại tất cả các dữ kiện liên quan đến stride. Ví dụ, nếu một kernel xác định được một khuôn mặt sau khi di chuyển một bước sang bên phải, chỉ số đầu ra sẽ cho chúng ta biết tọa độ góc trên bên trái của kernel đó tại (1,0). Tuy nhiên, do stride là 2, chúng ta sẽ phải nhân chỉ với 2 để được tọa độ chính xác: (2,0).

Mỗi kernel sẽ nhỏ hơn nếu nằm trong một bức ảnh lớn hơn, do đó nó sẽ tìm được các khuôn mặt nhỏ hơn trong bức ảnh có kích thước lớn hơn. Tương tự, kernel sẽ lớn hơn nếu nằm trong một bức ảnh có kích thước nhỏ hơn, do đó có thể tìm được khuôn mặt to trong những bức ảnh kích thước nhỏ.

![Stage 1: P-Net](https://images.viblo.asia/c2ad8543-8c46-4485-a92c-ca3a8b893b7a.png)

Sau lớp convolution thứ 3, mạng chia thành 2 lớp. Convolution 4-1 đưa ra xác suất của một khuôn mặt nằm trong mỗi bounding boxes, và Convolution 4-2 cung cấp tọa độ của các bounding boxes.

Và dưới đây là output:

![Output cho P-Net. Chú ý rằng, output thực sự có 4 chiều, tuy nhiên để đơn giản thì ở đây tác giả đã kết hợp thành một mảng 2 chiều. Đồng thời, tọa độ của các bouding boxes là giá trị nằm giữa 0 và 1: (0,0) là góc trên bên trái của kernel, (1,1) là góc dưới bên phải của kernel.](https://cdn-images-1.medium.com/max/800/1*5jhmkluZtYXUbzfi-cplIQ.png)

Các ***weights*** và ***biases*** của P-Net đã được huấn luyện nên nó có thể cho ra các bouding box tương đối chính xác cho mỗi kernel 12x12. Tuy nhiên, trong số các bouding boxes đó có một số boxes mà mạng cho rằng có xác suất cao hơn, hay nó dự đoán chắc chắn hơn. Vì thế, chúng ta cần đưa output của P-Net về một list chứa các mứa độ chắc chắc của mỗi bouding box, và sau đó xóa đi các boxes với ***confidence*** thấp. 

![](https://cdn-images-1.medium.com/max/800/1*Ml1270kY5DoN4vU9Uwwi-A.png)

Sau khi chọn ra các boxes với ***confidence*** cao nhất, chúng ta sẽ phải chuẩn hóa hệ tọa độ, chuyển tất cả các hệ tọa độ về dạng thật của nó, ở bức ảnh gốc khi chưa được scaled sang các kích cỡ khác nhau. Vì phần lớn các kernels đang ở trong các bức ảnh được scale xuống các kích cỡ thấp hơn nên tọa độ của chúng đang được dựa vào các bức ảnh nhỏ hơn.

Tuy nhiên vẫn còn rất nhiều các bouding boxes trong bức ảnh, và rất nhiều trong số chúng nằm trùng lên nhau. Ở đây, ta sẽ sử dụng ***Non-Maximum Suppression***, hay NMS, là phương pháp để giảm số lượng các bouding boxes nằm trùng lên nhau.
Bạn đọc có thể tìm hiểu thêm về phương pháp này tại [đây](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) hoặc [đây](https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/).

![Non-Maximum Suppression](https://cdn-images-1.medium.com/max/800/1*Ml1270kY5DoN4vU9Uwwi-A.png)

Chúng ta thực hiện NMS cho tất cả các bức ảnh được scale, rồi thêm một lần nữa cho tất cả các kernels ở mỗi scale. Việc này giúp bỏ đi các bouding boxes thừa, cho phép chúng ta thu hẹp phạm vi tìm kiếm xuống còn một box chính xác nhất cho mỗi khuôn mặt.

Có thể đến đây sẽ có nhiều người hỏi rằng tại sao không chọn luôn box với ***confidence*** cao nhất và xóa đi tất cả những cái còn lại? Trong bức ảnh trên của chúng ta chỉ có một khuôn mặt, tuy nhiên có thể trong các bức ảnh khác lại có nhiều khuôn mặt hơn. Vì vậy, chúng ta có thể sẽ xóa đi bouding boxes của tất cả các khuôn mặt khác có trong bức ảnh. 

![Thực hiện đưa bức ảnh về kích thước gốc](https://cdn-images-1.medium.com/max/800/1*7JWQP4FhkMxuCfzSsfFR_A.png)

Trong bức ảnh trên, box màu đỏ biểu diễn kernel 12x12, được resized về bức ảnh gốc ban đầu. Chúng ta có thể tính được chiều rộng và chiều dài của kernel: 500 - 200 = 300, 1800 - 1500 = 300 ( Để ý rằng chiều dài và chiều rộng của kernel không nhất thiết phải là 12. Đó là bởi vi ta đang sử dụng tọa độ của kernel trong bức ảnh gốc. Chiều dài và chiều rộng của kernel ở đây là khi đã được đưa về kích thước của bức ảnh gốc) Sau đó, chúng ta nhân tọa độ của bounding box màu vàng với 300: 0.4x300 = 120, 0.2x300=60, 0.9x300=270, 0.7x300=210. Để ý ở đây tọa độ của bounding box đã được chuẩn hóa về giữa 0 và 1 so với hệ tọa độ trong kernel. Cuối cùng, ta cộng tọa độ phía trên bên trái của kernel để được tọa độ chính xác của bouding box: (200+120,500+60) và (200+270,500+210) hay (320,560) và (470,710).

Vì bounding box có thể không là hình vuông, nên chúng ta sẽ reshape chúng về hình vuông bằng cách kéo dài cạnh nhỏ hơn ra. Sau đó, ta lưu lại t
